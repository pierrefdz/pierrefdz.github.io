---
layout: publication_page
show: true
noheader: true

title: 'The Stable Signature: Rooting Watermarks in Latent Diffusion Models'
description: 

date: 2023-03-08

authors:
  - name: Pierre Fernandez
    url: "https://pierrefdz.github.io/"
    affiliations: [Meta AI, Inria]
  - name: Guillaume Couairon
    url: "https://scholar.google.com/citations?user=O1DeDyEAAAAJ&hl=en"
    affiliations: [Meta AI]
  - name: Hervé Jégou
    url: "https://scholar.google.com/citations?user=1lcY2z4AAAAJ&hl=en"
    affiliations: [Meta AI]
  - name: Matthijs Douze
    url: "https://scholar.google.com/citations?user=0eFZtREAAAAJ&hl=en"
    affiliations: [Meta AI]
  - name: Teddy Furon
    url: "https://scholar.google.com/citations?hl=en&user=aLUbWzAAAAAJ"
    affiliations: [Inria]

journal: preprint (under review)
bib: /assets/bibliography/stablesignature.txt
pdf: /assets/publis/stablesignature/paper.pdf 
img: /assets/publis/stablesignature/splash.png

header-includes:
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

## TL;DR (Summary)

Recent progress in image generation has made it easy to create and manipulate images in a realistic manner with models like DALL·E~2 and Stable Diffusion. 
It raises concerns about responsible deployment of these models.

Stable Signature is a watermarking technique that modifies the generative model such that all images it produces hide an invisible signature.
These signatures can be used to detect and track the origin of images generated by latent diffusion models.

## How does it work?


### Image generation with Latent Diffusion Models

Latent Diffusion Models ([LDMs](https://arxiv.org/abs/2112.10752)) are generative models that produce images by using diffusion processes in the latent space of a variational auto-encoder (like VQ-GAN).
The diffusion process is guided by a text prompt, which is used to control the generation of latents.
The images are generated by decoding the latents at the end of the diffusion process, with the help of the VAE's decoder  $$D$$.

<img src="/assets/publis/stablesignature/generation.gif" class="img-fluid thumbnail mt-2" alt="LDM generation - overview">

### Stable Signature overview

By fine-tuning the decoder of the VAE, we can slightly modify all generated images, such that they hide an invisible signature.
In our case, Alice fine-tunes the decoder to produce images that hide a 48-bits binary signature. 
As an example use-case, she can later give her signature to content sharing platforms, allowing them to block content generated by her model.

<img src="/assets/publis/stablesignature/stablesign_anim.gif" class="img-fluid thumbnail mt-2" alt="Stable Signature - overview">

**Remarks**

Due to potential concerns associated with pre-existing third-party generative models, such as Stable Diffusion or LDMs, we refrain from experimenting with these models and instead use a large diffusion model (2.2B parameters) trained on a proprietary dataset of 330M image-text pairs.
However, the variational auto-encoder of our LDM is the same as Stable Diffusion (the one used in LDM with a compression factor of $$F=8$$).


### Stable Signature method



#### Watermark pre-training

We use jointly train 2 simple convolutional neural networks, in the same way as [HiDDeN](https://arxiv.org/abs/1807.09937). 
The first one (the watermark encoder $$E$$) takes as input an image $$x$$ and a message $$m$$ and outputs a new watermark image $$x'$$.
$$x'$$ is then subject to image augmentations such as crops, flips, or JPEG compression.
The second one, i.e. the watermark extractor $$W$$, takes as input $$x'$$ and outputs the message $$m'$$.
The goal is to train $$E, W$$ such that $$m' = m$$.

At the end of this training we discard $$E$$ that does not serve our purpose and only keep $$W$$.


#### Fine-tuning the decoder
Then, we fine-tune the decoder $$D$$ of the VAE into $$D_m$$ to produce images that hide a signature $$m$$ (note that all networks are fixed unless $$D_m$$).
This is inspired by the training procedure of [LDM](https://arxiv.org/abs/2112.10752).
During a loop of the fine-tuning, a batch $$x$$ of training images is encoded, then decoded by $$D$$, resulting in images $$x'$$.
The loss to be minimized is then computed as a weighted sum of the message loss $$L_{m}$$, which aims at minimizing the distance between the extracted message $$m'$$ and the target message $$m$$, and the perceptual image loss $$L_{i}$$, which aims at generating images close to the original ones.

<img src="/assets/publis/stablesignature/method.svg" class="img-fluid thumbnail mt-2" alt="Corgi and T-rex generated with and without watermarked LDMs">


## Main results

### Images

Examples of images generated by LDMs from text prompts with and without watermarking with Stable Signature.
*Left*: we use the default decoder, without watermark fine-tuning.
*Middle*: we use the decoder with watermark fine-tuning.
*Right*: We show the difference, multiplied by 10 because the distortion is hard to perceive.

<img src="/assets/publis/stablesignature/qual.svg" class="img-fluid thumbnail mt-2" alt="Corgi and T-rex generated with and without watermarked LDMs">


### Detection and identification





## Links

- [`PDF`]({{page.pdf}})
- [`BibTeX`]({{page.bib}})